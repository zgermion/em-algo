\documentclass[russian]{vegapresentation}


\DeclareMathOperator*{\cor}{\operatorname{corr}}
\DeclareMathOperator*{\plim}{\ensuremath{\operatorname{\P-lim}}}



\subtitle{Студенческая научная группа "Факторный анализ и прогнозирование"}
\title{Байесовские методы. EM-алгоритм. Общие сведения}
\author{Зеленин Герман}
\institute{Московский государственный университет, механико-математический факультет}
\date{12 ноября 2022}

\begin{document}
    \maketitle
    
    \section{Основные понятия}
    
    \begin{frame}{Основные понятия}
		\begin{definition}
			Пусть $x$ и $y$ - две случайные величины. Тогда \textbf{условным распределением} $p(x|y)$ $x$ относительно $y$ называется отношение \textit{совместного распределения} $p(x,y)$ и \textit{маргинального распределения} $p(x)$:
			\begin{equation}
				p(x|y) = \frac{p(x,y)}{p(y)}
			\end{equation}
		\end{definition}
    \end{frame}
    
    \begin{frame}{Основные понятия}
		\begin{theorem}
			Пусть $x_1,x_2,...,x_n$ - случайные величины. Тогда их совместное распределение можно представить в виде произведения $n$ одномерных условных распределений с постоянно уменьшающейся посылкой:
			\begin{equation}
				p(x_1, ..., x_n) = p(x_n|x_1, ..., x_{n-1})...p(x_2|x_1)p(x_1)
			\end{equation}
		\end{theorem}
    \end{frame}

    \begin{frame}{Основные понятия}
		\begin{theorem}
			Пусть $x_1,x_2,...,x_n$ - случайные величины. Если известно их совместное распределение $p(x_1, ..., x_n)$, то совместное распределение подмножества случайных величин $x_1, ..., x_k$ будет равно:
			\begin{equation}
				p(x_1, ..., x_k) = \int p(x_1, ..., x_n)dx_{k+1}...dx_n
			\end{equation}
		\end{theorem}
    \end{frame}
    
    \begin{frame}{Основные понятия}
		\begin{definition}
			Будем говорить, что распределение $p(x|\theta)$ лежит в \textbf{экспоненциальном классе}, если оно может быть представлено в следующем виде:
			\begin{equation}
				p(x|\theta) = \frac{f(x)}{g(\theta)}exp(\theta^Tu(x))
			\end{equation}
		\end{definition}
    \end{frame}
    
    \section{Частотный и байесовский подход}

    \begin{frame}{Частотный подход. Метод максимального правдоподобия}
        Рассмотрим некоторую выборку $X=(X_1, X_2, ..., X_n)$ из некоторого параметрического распределения $p_\theta(x)$ \\
        \vspace{3mm}
        \textbf{Задача:} хотим оценить параметр $\theta$ так, чтобы вероятность пронаблюдать то, что мы пронаблюдали, была максимальна. \\
        \begin{equation}
                    \theta_{ML} = \arg\max_{\theta}p(X|\theta) = \arg\max_{\theta} \prod\limits_{i=1}^{n}p(x_i|\theta) = \arg\max_{\theta} \sum\limits_{i=1}^{n}\log p(x_i|\theta)
         \end{equation}
    \end{frame}
    
    \begin{frame}{Частотный подход. Метод максимального правдоподобия}
		Во многих частных случаях сумма логарифмов правдоподобий будет выпуклой вверх функцией, то есть у неё один максимум, который достаточно легко найти даже в пространствах высокой размерности. \\
		\vspace{3mm}
		Заметим, что $\theta_{ML}$ - случайная величина, так как является функцией от выборки.
    \end{frame}
    
	\begin{frame}{Свойства оценки максимума правдоподобия}
		\begin{enumerate}
         	\item Состоятельность: ОМП сходится к истинному значению параметров по вероятности при $n\rightarrow+\infty$,
			\item Асимптотически несмещенная: $\theta_{ML} = E[\theta]$ при $n\rightarrow+\infty$,
			\item Асимптотически нормальная: $\theta_{ML}$ распределена нормально при $n\rightarrow+\infty$
			\item Асимптотическая эффективность: ОМП обладает наименьшей дисперсией среди всех состоятельных асимптотически нормальных оценок.
         \end{enumerate}
    \end{frame}
    
    	\begin{frame}{Чем частотный подход плох в машинном обучении?}
		Как уже было сказано, в методе максимального правдоподобия мы выбираем параметр распределения так, чтобы вероятность пронаблюдать то, что мы пронаблюдали была максимальной. Говоря на языке машинного обучения, мы подгоняем параметры распределения под полученные данные, а это чревато переобучением.
    \end{frame}
    
    \begin{frame}{Байесовский подход. Теорема Байеса}
     	\begin{theorem}
     		Пусть $x$ и $y$ - случайные величины. Тогда \\
     		\begin{equation}
     			p(x|y) = \frac{p(x|y)p(y)}{\int p(x|y)p(y)dy}
     		\end{equation} 
     		где $p(y|x)$ - апостериорное распределение, $p(x|y)$ - правдоподобие, $p(y)$ - априорное распределение.
     	\end{theorem}
    \end{frame}
    
    \begin{frame}{Байесовский подход}
		Пусть у нас есть априорное распределение $p(\theta)$, которое отражает некую внешнюю информацию о возможных значениях параметров (если такой информации нет, мы всегда можем ввести неинформативное распределение). Тогда результатом применения теоремы Байеса будет апостериорное распределение на параметры:
		\begin{equation}
                    p(\theta|X) = \frac{\prod p(x_i| \theta)p(\theta)}{\int \prod p(x_i|\theta)p(\theta)d\theta}
         \end{equation}
         Ответом является новое распределение на параметры модели, в отличии от метода максимального правдоподобия, где ответом являлось конкретное значение параметров.
    \end{frame}
    
    \section{EM-алгоритм}
    \begin{frame}{EM-алгоритм. Формулировка решаемой задачи}
		\textbf{Задача 1:} По выборке $X$ восстановить параметры $\theta$  распределения методом максимального правдоподобия:
		\begin{equation}
			p(X|\theta) \rightarrow \max_\theta
		\end{equation}
		\textbf{Вопрос: } В каких параметрических семействах эту задачу можно решать эффективно?
    \end{frame}

    \begin{frame}{EM-алгоритм. Формулировка решаемой задачи}
		\textbf{Ответ:} Если плотность распределения $p(X |\theta)$ лежит в экспоненциальном классе, то мы можем эффективно найти оценку максимального правдоподобия для параметров $\theta$. Иногда это возможно в явном виде (дифференцируем логарифм правдоподобия, приравниваем к нулю, и находим из полученной системы уравнений параметры $\theta$), а в остальных случаях можно построить эффективную численную процедуру оценки (благодаря тому, что логарифм функции правдоподобия — вогнутая функция).
    \end{frame}
    
    \begin{frame}{EM-алгоритм. Формулировка решаемой задачи}
		Проблема заключается в том, что экспоненциальный класс не такой широкий, как могло бы показаться. Зачастую на практике наблюдаемые данные имеют гораздо более сложное распределение, которое в экспоненциальный класс никак не вписывается. \\
		\vspace{3mm}
		\textbf{Пример: } смесь гауссиан \\
		\vspace{3mm}
		Наши данные пришли из сложного распределения. Но если бы мы знали что-нибудь еще, то наше распределение стало бы куда более простым.
    \end{frame}
    
    \begin{frame}{EM-алгоритм. Формулировка решаемой задачи}
		\textbf{Задача 2: } Введем латентные переменные $Z$ так, чтобы совместное распределение $p(X, Z|\theta)$ лежало в экспоненциальном классе. Вместо предыдущей задачи, будем решать следующую задачу:
		\begin{equation}
			p(X,Z|\theta) \rightarrow \max_\theta
		\end{equation}
		\textbf{Замечание: } Помимо нахождения решения исходной задачи, мы найдем так же информацию по латентным переменным. 
    \end{frame}

    \begin{frame}{Вывод EM-алгоритма}
    		Записываем цепочку преобразований:
		\begin{multline}
    			\log p(X|\theta) = \int q(Z) \log p(X|\theta)dZ =\\= \int \log \frac{p(X,Z|\theta)}{p(Z|X,\theta)}dZ =\int \log \frac{p(X,Z|\theta)q(Z)}{p(Z|X,\theta)q(Z)}dZ =\\= \int q(Z) \log \frac{p(X,Z|\theta)}{q(Z)}dZ + \int q(Z) \log \frac{q(Z)}{p(Z|X, \theta)}dZ
		\end{multline}
		где $p(q)$ - произвольное распределение в пространстве латентных переменных.
    \end{frame}
    
    \begin{frame}{Дивергенция Кульбака - Лейбера}
		\begin{definition}
			\textit{Дивергенция Кульбака-Лейбера} между двумя распределениями $p$ и $q$ определяется следующим образом: 
			\begin{equation}
				KL(p(x) || q(x)) = \int p(x) \log \frac{p(x)}{q(x)} dx
			\end{equation}
		\end{definition}
		
		\begin{theorem}
			$KL(p||q) \geqslant 0$, причем $KL(p||q) = 0$ если и только если эти распределения почти всюду (везде кроме множества меры ноль) совпадают.
		\end{theorem}
    \end{frame}
    
	\begin{frame}{Вывод EM-алгоритма}
    		Заметим, что в выражении (10) второе выражение является KL-дивергенцией распределений $q(Z)$ и $p(Z|X, \theta)$. Так как она неотрицательна, то можем записать следующее неравенство:
    		\begin{equation}
    			\log p(X|\theta) \geqslant \int q(Z) \log \frac{p(X, Z| \theta)}{q(Z)} dZ
    		\end{equation}
    		
    		Идея EM-алгоритма состоит в том, чтобы вместо оптимизации логарифма неполного правдоподобия оптимизировать полученную нижнюю оценку, но теперь уже как по $\theta$ так и по распределению $q$
    	\end{frame}
    	
    	\begin{frame}{Вывод EM-алгоритма}
		\begin{definition}
			Правая часть выражения 12 называется \textit{нижней границей на обоснованность (ELBO)} и обозначается как $\mathcal{L}(q, \theta)$
		\end{definition}
		Нижняя оценка на обоснованность является вариационной нижней оценкой, то есть удовлетворяет тому, что всегда не превосходит выражения, которое оценивает (что как раз говорит выражение 12), а так же для любого аргумента исходной функции $(\theta)$ найдутся такие значения вариационных $(q)$, для которых неравенство превращается в равенство.
    	\end{frame}
    	
	\begin{frame}{Вывод EM-алгоритма}
		Воспользуемся этими свойствами и перейдем от оптимизации неполного правдоподобия к оптимизации нижней оценки на обоснованность. Будем решать данную задачу итерационно: \\
		1) Оптимизировать по $q$ при фиксированном $\theta$ (E-шаг):
		\begin{equation}
			\mathcal{L}(q, \theta_0) \longrightarrow \max_{q} \Rightarrow q(Z) = p(Z|X, \theta)
		\end{equation}
		2) Оптимизировать по $\theta$ при фиксированном $q$ (M-шаг):
		\begin{equation}
			\mathcal{L}(q_0, \theta) \longrightarrow \max_{\theta} \Leftrightarrow \int q(Z) \log p(X, Z| \theta)dZ \longrightarrow \max_{\theta}
		\end{equation}
    	\end{frame}
    	
    	
	\begin{frame}{Вывод EM-алгоритма}
		На E-шаге задача функциональной оптимизации. Сумма в (10) не зависит от $q$, а потому максимизация по $q$ первого слагаемого эквивалентна минимизации по $q$ второго, а второе слагаемое - KL-дивиргенция. Мы знаем, что она достигает минимума, следовательно, приравниваем $q(Z) = p(Z|X, \theta)$. Если можем найти апостериорное распределение $p(Z| X, \theta)$, то E-шаг проделывается в явном виде.
    	\end{frame}
    	
    	\section{Области применения алгоритма:}
    	
    	\begin{frame}{Вывод EM-алгоритма}
		1) Разделение смесей распределения (латентные переменные - номера распределений, из которых пришли данные). \\
		2) Метод главных компонент - метод уменьшения размерности данных, потеряв наименьшую информацию. \\
		3) Задачи классификации.
    	\end{frame}
    	
    	
    

\end{document}
